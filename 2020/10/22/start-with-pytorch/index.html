<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="记录，分享。">
    <meta name="author" content="徐徐">
    
    <title>
        
            Pytorch入门 |
        
        一通胡编
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/favicon-32x32-next.png">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/css/font-awesome.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"machacroissant.github.io","root":"/","language":"zh-CN","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/tuotuo.jpeg","favicon":"/images/favicon-32x32-next.png","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":true},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"记录，分享。"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":false},"code_copy":{"enable":true,"style":"mac"},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.3"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="来个抹茶可颂" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                一通胡编
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">Pytorch入门</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/tuotuo.jpeg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">徐徐</span>
                        
                            <span class="author-label">抱歉选手</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;2020-10-22 18:50:03
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/deeplearning/">deeplearning</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/ML/">ML</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/DL/">DL</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>7 分钟</span>
        </span>
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p><strong>机器学习</strong>研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是<strong>实现人工智能的一种手段，人工智能是最终目的</strong>。</p>
<p>在机器学习的众多研究方向中，<strong>表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出</strong>。</p>
<p><strong>深度学习是具有<em>多级表示</em>的表征学习方法。</strong>在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，<em>深度学习模型也可以看作是由许多简单函数复合而成的函数。</em>当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。</p>
<p>深度学习可以<strong>逐级表示越来越抽象的概念或模式</strong>。</p>
<p>深度学习的一个外在特点是<strong>端到端的训练</strong>。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。</p>
<p>除端到端的训练以外，我们也正在经历<strong>从含参数统计模型转向完全无参数的模型</strong>。</p>
<p>任务可以分为三种：回归Regression(线性模型)，分类classification(非线性模型，包括深度学习、SVM等)，结构化学习structured learning。</p>
<p>实现的手段可以有：监督学习、半监督学习、无监督学习、强化学习。相同点是统一都有输入，区别在于是否有输入的匹配输出。从前往后输入输出正确匹配对以此减少。</p>
<h1 id="Tensor与Numpy"><a href="#Tensor与Numpy" class="headerlink" title="Tensor与Numpy"></a>Tensor与Numpy</h1><h2 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>PyTorch<strong>操作inplace版本</strong>都有后缀<code>_</code>, 例如<code>x.copy_(y), x.t_()</code></p>
<p><strong>判断操作是否开辟新内存</strong>，使用Python自带的<code>id</code>函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。</p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p>torch.tensor()与np.array()用法类似，创建方法类似，二者可以转换，可接受基本运算法。</p>
<p>四种形式的加法(+, add(), add_(), +=)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>Torch</th>
<th>Numpy</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建</td>
<td>torch.tensor([0.1, 0.2, 0.3])</td>
<td>np.array([0.1, 0.2, 0.3])</td>
<td></td>
</tr>
<tr>
<td>转换</td>
<td>torch.from_numpy(x_numpy)</td>
<td>x_torch.numpy()</td>
<td>这两个函数所产生的<code>Tensor</code>和NumPy中的数组共享相同的内存（所以他们之间的转换很快）。</td>
</tr>
<tr>
<td>四则运算</td>
<td>x_torch + y_torch</td>
<td>x_numpy + y_numpy</td>
<td></td>
</tr>
<tr>
<td>范数</td>
<td>torch.norm(x_torch)</td>
<td>np.linalg.norm(x_numpy)</td>
<td></td>
</tr>
<tr>
<td>正则化</td>
<td>torch.mean(x_torch, dim=0)</td>
<td>np.mean(x_numpy, axis=0)</td>
<td></td>
</tr>
<tr>
<td>索引</td>
<td></td>
<td></td>
<td>我们还可以使用类似NumPy的索引操作(:)来访问<code>Tensor</code>的一部分，不会开辟内存。需要注意的是：<strong>索引出来的结果与原数据共享内存。</strong></td>
</tr>
</tbody>
</table>
</div>
<p>将NumPy中的array转换成<code>Tensor</code>的另一个方法就是<code>torch.tensor()</code>, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的<code>Tensor</code>和原来的数据不再共享内存。</p>
<p>函数<code>item()</code>可以将一个标量<code>Tensor</code>转换成一个Python number。</p>
<h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Tensor</th>
<th>Numpy</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tensor.view()</td>
<td>np.shape()</td>
<td>Torch.view() can also automatically calculate the correct dimension if a <code>-1</code> is passed in. This is useful if we are working with batches, but the batch size is unknown. Torch.view()返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>注意<code>view()</code>返回的新<code>Tensor</code>与源<code>Tensor</code>虽然可能有不同的<code>size</code>，但是是共享<code>data</code>的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)</strong>。</p>
<p>虽然<code>view</code>返回的<code>Tensor</code>与源<code>Tensor</code>是共享<code>data</code>的，但是依然是一个新的<code>Tensor</code>（因为<code>Tensor</code>除了包含<code>data</code>外还有一些其他属性），二者id（内存地址）并不一致。</p>
<p>想返回一个真正新的副本（即不共享data内存）该用<code>clone</code>创造一个副本然后再使用<code>view</code>。view和copy存在区别。</p>
<h1 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h1><p>遵循以下两个原则：</p>
<ul>
<li>Each tensor has at least one dimension.</li>
<li>When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.</li>
</ul>
<p>对应dimension要么相同，要么一个是1另一个是更大的数字。</p>
<h1 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h1><p>为什么需要用到计算图？<a class="link" target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">参考<i class="fas fa-external-link-alt"></i></a></p>
<p>计算图将函数表达式用树状结构表示。求偏导，联系链式法则，因子路径。</p>
<p>对于某一个结点往根结点去就是forward propagation，可以求该节点对它上面的所有节点的影响。对于某一个节点往叶子结点去就是backpropagation，可以求该节点受它下面的那些节点的影响。</p>
<p>记录了计算图，就能追踪每一次微分。</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="硬件支持"><a href="#硬件支持" class="headerlink" title="硬件支持"></a>硬件支持</h1><p>用方法<code>to()</code>可以将<code>Tensor</code>在CPU和GPU（需要硬件支持）之间相互移动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gpu = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">x.to(gpu)</span><br></pre></td></tr></table></figure>
<h1 id="批量数据"><a href="#批量数据" class="headerlink" title="批量数据"></a>批量数据</h1><p>主要涉及两个工具</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure>
<h2 id="custom-dataset"><a href="#custom-dataset" class="headerlink" title="custom dataset"></a>custom dataset</h2><p>Your custom dataset should inherit <code>Dataset</code> and override the following methods:</p>
<ul>
<li><code>__len__</code> so that <code>len(dataset)</code> returns the size of the dataset.</li>
<li><code>__getitem__</code> to support the indexing such that <code>dataset[i]</code> can be used to get 𝑖\ th sample</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FakeDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> len(self.x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br></pre></td></tr></table></figure>
<h1 id="data-loader"><a href="#data-loader" class="headerlink" title="data loader"></a>data loader</h1><p>实现数据批量处理：batch/shuffling/parallel loading。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = FakeDataset(x, y)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="反向传播与自动求梯度"><a href="#反向传播与自动求梯度" class="headerlink" title="反向传播与自动求梯度"></a>反向传播与自动求梯度</h1><h2 id="追踪梯度的步骤"><a href="#追踪梯度的步骤" class="headerlink" title="追踪梯度的步骤"></a>追踪梯度的步骤</h2><ol>
<li><p>对input tensor x的定义<code>.requires_grad</code>属性设置为<code>True</code></p>
</li>
<li><p>调用反向传播<code>y.backward()</code>，来完成所有梯度计算。此input的<code>Tensor</code>的梯度将累积到Tensor类的<code>.grad</code>属性中。</p>
</li>
<li><p>随后用gradient descent更新x</p>
</li>
<li><p>然后需要把每一次反向传播后的梯度清零，因为.grad的数据不会被覆盖只会在前一个基础之上累加。</p>
<p>We need to zero the grad variable since the backward call accumulates the gradients in .grad instead of overwriting.</p>
<p>为了方便操作先调用调用<code>.detach()</code>将其从追踪记录中分离出来；再调用<code>x.grad.zero_()</code>清零梯度。</p>
</li>
</ol>
<h2 id="其他自动求梯度的设置"><a href="#其他自动求梯度的设置" class="headerlink" title="其他自动求梯度的设置"></a>其他自动求梯度的设置</h2><p>by using <code>.detach()</code> to get a new Tensor with the same content but that does not require gradients:</p>
<p> stop autograd from tracking history on Tensors with <code>.requires_grad=True</code> either by wrapping the code block in <code>with torch.no_grad():</code></p>
<h2 id="构造复杂模块"><a href="#构造复杂模块" class="headerlink" title="构造复杂模块"></a>构造复杂模块</h2><p>Many times, we want to compose Modules together. <code>torch.nn.Sequential</code> provides a good interface for composing simple modules.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Sequential(</span><br><span class="line">                            nn.Linear(d_in, d_hidden),</span><br><span class="line">                            nn.Tanh(),</span><br><span class="line">                            nn.Linear(d_hidden, d_out),</span><br><span class="line">                            nn.Sigmoid()</span><br><span class="line">                           )</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>两种常见的loss function。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>训练模型需要用到求梯度的几种方法，比如SGD之类的。<code>torch.optim</code>提供了求梯度的一些方法，至少要传入<strong>模型参数和学习率</strong>。</p>
<p>PyTorch implements a number of gradient-based optimization methods in <code>torch.optim</code>, including Gradient Descent. At the minimum, it takes in the model parameters and a learning rate.</p>
<p>在一个关于data_size/batch_size的循环，或者data_size的循环中，首先计算y_hat，其次计算loss，接下来求梯度(<strong>Optimizers do not compute the gradients for you, so you must call <code>backward()</code> yourself. </strong>)，要注意先清空梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    <span class="comment"># shuffling data</span></span><br><span class="line">    rand_idx = np.random.choice(n) <span class="comment"># take a random point from the dataset</span></span><br><span class="line">    x = X[rand_idx] </span><br><span class="line">    y_hat = linear_module(x)</span><br><span class="line">    loss = loss_func(y_hat, y)</span><br><span class="line">    <span class="comment"># You also must call the optim.zero_grad() function before calling backward() since by default PyTorch does and inplace add to the .grad member variable rather than overwriting it.</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br></pre></td></tr></table></figure>
<p>在初始化参数的时候可以定一个momentum的初始值，之后当作参数传入<code>torch.optim</code>。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a class="link" target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=kQeezFrNoOg&amp;feature=youtu.be">台大李宏毅机器学习助教补充课<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">Pytorch官方文档<i class="fas fa-external-link-alt"></i></a></p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li>本文标题：Pytorch入门</li>
        <li>本文作者：徐徐</li>
        <li>创建时间：2020-10-22 18:50:03</li>
        <li>
            本文链接：https://machacroissant.github.io/2020/10/22/start-with-pytorch/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2020/10/22/ml-env-setup/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">PyTorch+Anaconda+JupyterNotebook环境搭建</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2020/10/20/github-up-down/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Github使用操作以及问题</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script 
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script >
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'aqAOPQAx86speqYXI4gPuLWx-gzGzoHsz',
                    appKey: '8fzaXFq8Ikhr5mucbhj3qqUS',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'Let&#39;s talk!',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = '徐徐';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('false') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>&nbsp;-&nbsp;
            
            2021&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">徐徐</a>
        </div>
        
            <script async  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                
                    <span id="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.3</a>
        </div>
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensor%E4%B8%8ENumpy"><span class="nav-number">2.</span> <span class="nav-text">Tensor与Numpy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E5%8C%85"><span class="nav-number">2.1.</span> <span class="nav-text">导入包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="nav-number">2.2.</span> <span class="nav-text">基本操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape"><span class="nav-number">2.3.</span> <span class="nav-text">Reshape</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Broadcasting"><span class="nav-number">3.</span> <span class="nav-text">Broadcasting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">4.</span> <span class="nav-text">计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.1.</span> <span class="nav-text"> </span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A1%AC%E4%BB%B6%E6%94%AF%E6%8C%81"><span class="nav-number">5.</span> <span class="nav-text">硬件支持</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE"><span class="nav-number">6.</span> <span class="nav-text">批量数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#custom-dataset"><span class="nav-number">6.1.</span> <span class="nav-text">custom dataset</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#data-loader"><span class="nav-number">7.</span> <span class="nav-text">data loader</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="nav-number">8.</span> <span class="nav-text">反向传播与自动求梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%BD%E8%B8%AA%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">8.1.</span> <span class="nav-text">追踪梯度的步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%BE%E7%BD%AE"><span class="nav-number">8.2.</span> <span class="nav-text">其他自动求梯度的设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%A4%8D%E6%9D%82%E6%A8%A1%E5%9D%97"><span class="nav-number">8.3.</span> <span class="nav-text">构造复杂模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">8.4.</span> <span class="nav-text">定义损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.5.</span> <span class="nav-text">训练模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/dark-light-toggle.js"></script>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/local-search.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/code-copy.js"></script>




<div class="post-scripts">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/left-side-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/toc.js"></script>
    
</div>



</body>
</html>
