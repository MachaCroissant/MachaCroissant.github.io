<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="code-TtS62HQLaD">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="李宏毅机器学习无监督学习，聚类部分。降维方法涉及特征选择、主成分分析、矩阵分解、word-embedding、Neighbour Embedding(LLE&#x2F;T-SNE&#x2F;LE)、Auto-Encoder。">
<meta property="og:type" content="article">
<meta property="og:title" content="无监督学习">
<meta property="og:url" content="http://example.com/2020/11/06/unsupervised-learning/index.html">
<meta property="og:site_name" content="来个抹茶可颂">
<meta property="og:description" content="李宏毅机器学习无监督学习，聚类部分。降维方法涉及特征选择、主成分分析、矩阵分解、word-embedding、Neighbour Embedding(LLE&#x2F;T-SNE&#x2F;LE)、Auto-Encoder。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-11-06T01:13:07.000Z">
<meta property="article:modified_time" content="2020-11-13T05:41:05.824Z">
<meta property="article:author" content="徐徐">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="无监督学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2020/11/06/unsupervised-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>无监督学习 | 来个抹茶可颂</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">来个抹茶可颂</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/unsupervised-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="徐徐">
      <meta itemprop="description" content="记录，分享。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="来个抹茶可颂">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          无监督学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-06 09:13:07" itemprop="dateCreated datePublished" datetime="2020-11-06T09:13:07+08:00">2020-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-13 13:41:05" itemprop="dateModified" datetime="2020-11-13T13:41:05+08:00">2020-11-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>
            <div class="post-description">李宏毅机器学习无监督学习，聚类部分。降维方法涉及特征选择、主成分分析、矩阵分解、word-embedding、Neighbour Embedding(LLE/T-SNE/LE)、Auto-Encoder。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>聚类的几种常用方法：K-means，Hierarchical Agglomerative Clustering(HAC)。</p>
<p>K-means需要事先决定有K个cluster，每个cluster初始的center也要从training data中随机找k个出来。</p>
<p>Hierarchical Agglomerative Clustering(HAC)就是建树，对每个data两两计算相似度，挑出最相似的一对data，merge成一个新的data vector。一直从下至上构建出root。要分类就是在每一层切一刀获得一种聚类方式。</p>
<h1 id="Dimension-Reduction降维"><a href="#Dimension-Reduction降维" class="headerlink" title="Dimension Reduction降维"></a>Dimension Reduction降维</h1><p>降维的本质就是需要找一个function，当我们input一个vector x的时候，output是另外一个vector z，且output dimension小于input dimension。</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>把data的分布拿出来看一下，发现都集中在某个特定的dimension，那么就拿掉其他dimension就可以。但问题是很多时候任何一个dimension都不能拿掉。</p>
<h2 id="Principal-Component-Analysis-主成分分析"><a href="#Principal-Component-Analysis-主成分分析" class="headerlink" title="Principal Component Analysis 主成分分析"></a>Principal Component Analysis 主成分分析</h2><h3 id="一维"><a href="#一维" class="headerlink" title="一维"></a>一维</h3><p>$\boldsymbol{z} = \boldsymbol{W}\boldsymbol{x}$需要reduce to 1-D，就是要矩阵中的每一行的data points $x^i$投影到$w^1$，然后就可以得到对应的$z_1$，不同的 $x^i$投影过后都是相同的$z_1$。这里的$w^1$就是$\boldsymbol{W}$的第一行。</p>
<p>该如和确定这个$w^1$？我们希望投影后所有点之间的奇异度越高越好，对应到数学上就是$z_1$的方差越大越好。</p>
<script type="math/tex; mode=display">
Var(z_1) = \sum_{z_1}(z_1-\bar{z}_1)^2 \quad \left\|w^1\right\|_2=1</script><h3 id="高维"><a href="#高维" class="headerlink" title="高维"></a>高维</h3><p>现在想要投影到一个二位平面，除了把矩阵中的每一行的data points $x^i$投影到$w^1$，还要把矩阵中的每一行的data points $x^i$投影到$w^2$，这里的$w^1, w^2$是$\boldsymbol{W}$的第一行和第二行。</p>
<p>除了基向量的模为1的约束条件，还需要基向量之间是垂直的，代表基向量之间正交。因而可以说$\boldsymbol{W}$是一个正交矩阵orthogonal matrix。</p>
<h3 id="如何寻找正交基"><a href="#如何寻找正交基" class="headerlink" title="如何寻找正交基"></a>如何寻找正交基</h3><h4 id="w-1-的寻找"><a href="#w-1-的寻找" class="headerlink" title="$w^1$的寻找"></a>$w^1$的寻找</h4><script type="math/tex; mode=display">
\begin{split}
Var(z_1) &= \frac{1}{N}\sum_{z_1}(z_1-\bar{z}_1)^2 \\
&= \frac{1}{N}\sum_{x}(w^1\cdot x-w^1\cdot\bar{x})^2 \\
&= \frac{1}{N}\sum_{x}(w^1\cdot(x-\bar{x}))^2
\end{split}</script><p>向量内积可以和矩阵乘法转换（第一个等式），scalar做Transpose之后仍然是一个scalar（最后一个等式），获得如下式子：</p>
<script type="math/tex; mode=display">
(a\cdot b)^2 = (a^Tb)^2 = a^Tba^Tb=a^Tb(a^Tb)^T=a^Tbb^Ta</script><p>带入原方程得到</p>
<script type="math/tex; mode=display">
\begin{split}
Var(z_1) &= \frac{1}{N}\sum_{x}(w^1\cdot(x-\bar{x}))^2\\
&= \frac{1}{N}\sum_{x} (w^1)^T(x-\bar{x})(x-\bar{x})^Tw^1 \\
&= (w^1)^T \frac{1}{N}\sum_{x}(x-\bar{x})(x-\bar{x})^T w^1 \\
&= (w^1)^T Cov(x) w^1 \quad S = Cov(x)
\end{split}</script><p>也就是说我们需要找的$w^1$就是能够让$Var(z_1)$最大，且满足约束条件$\left|w^1\right|_2=(w^1)^Tw^1=1$的向量。约束条件是为了保证解出来的vector存在有限个。</p>
<p>运用Lagrange Multiplier拉格朗日数乘法</p>
<script type="math/tex; mode=display">
g(w^1) = (w^1)^T S w^1 - \alpha((w^1)^Tw^1-1)</script><p>对这个式子求偏导数$\partial(w^1)$，得到</p>
<script type="math/tex; mode=display">
Sw^1 = \alpha w^1 \\
(w^1)^TSw^1 = \alpha (w^1)^Tw^1 = \alpha</script><p>意思就是$w^1$是协方差矩阵$S$的特征向量，且它是对应到最大的特征值的那个特征向量。</p>
<h4 id="w-2-的寻找"><a href="#w-2-的寻找" class="headerlink" title="$w^2$的寻找"></a>$w^2$的寻找</h4><p>有两个约束条件，因而拉格朗日数乘法的对象如下</p>
<script type="math/tex; mode=display">
g(w^2) = (w^2)^T S w^2 - \alpha((w^2)^Tw^2-1) - \beta((w^2)^Tw^1-0)</script><p>对这个式子求偏导数$\partial(w^2)$，再同时乘以$(w^1)^T$，得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
S w^2 - \alpha w^2 - \beta w^1 = 0 \\
(w^1)^T  S w^2 -  (w^1)^T  \alpha w^2 - (w^1)^T \beta w^1 = 0 \\
((w^1)^T  S w^2)^T -\alpha \cdot 0 - \beta  \cdot 1 = 0 \\
\beta = ((w^1)^T  S w^2)^T = (w^2)^TSw^1= (w^2)^T \lambda_1 w^1 = 0
\end{aligned}</script><p>上面的推导表明$\beta=0$，那么最初的式子就可以写成</p>
<script type="math/tex; mode=display">
Sw^2=\alpha w^2</script><p>意思就是$w^2$是协方差矩阵$S$的第二大的特征向量，且它是对应到最第二大的特征值的那个特征向量。</p>
<h3 id="投影后数据的特征"><a href="#投影后数据的特征" class="headerlink" title="投影后数据的特征"></a>投影后数据的特征</h3><p>投影和的数据在新的坐标的各个维度之间是不相关的，在数学上的表示就是投影后的$\boldsymbol{z} = \boldsymbol{W}\boldsymbol{x}$的协方差矩阵是一个对角矩阵diagonal matrix。好处是实现模型的时候可以减少参数。</p>
<h3 id="从另一个角度理解PCA"><a href="#从另一个角度理解PCA" class="headerlink" title="从另一个角度理解PCA"></a>从另一个角度理解PCA</h3><p>一个输入input可以看作是有限个component的线性叠加加上所有input的平均值。</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
x = \bar{x} + \hat{x} \\
\hat{x} = c_1u^1+c_2u^2+ \cdots+c_Ku^K
\end{array}</script><p>Reconstruction error: $\left|(x-\bar{x}) - \hat{x}\right|_2$</p>
<p>Find $\{u^1, u^2 \cdots, u^k\}$ minimizing the error</p>
<script type="math/tex; mode=display">
L = min_{\{u^1, u^2 \cdots, u^k\}}\sum\left\|(x-\bar{x}) -(\sum_{k=1}^K{c_ku^k})\right\|_2</script><p>这里要找的$u^k$就是对应前面PCA中的$W$。但是这两者有有不同，因为前面PCA要求必须垂直（线性情况较快），但此处用gradient descent求解出来的对象不一定保证垂直（但是可以deep）。</p>
<p>PCA looks like a neuron network with one hidden layer(linear activation function).</p>
<p>PCA involves adding up and substracting some components, and then the componets may not be part of the input. 也就说是用PCA解得得结果不一定是直观上我们可以理解的东西，为了让eigen vector直观化，需要使用Non-negative matrix factorization（NMF），要么让线性叠加的系数为正，要们让线性叠加的子部component为正。</p>
<h3 id="弱点"><a href="#弱点" class="headerlink" title="弱点"></a>弱点</h3><p>有可能把两个class都放到一个vector上去。需要用到Linear Discriminant Analysis。</p>
<p>只适用于线性。</p>
<h1 id="矩阵分解-Matrix-Factorization"><a href="#矩阵分解-Matrix-Factorization" class="headerlink" title="矩阵分解 Matrix Factorization"></a>矩阵分解 Matrix Factorization</h1><p>推荐系统中，通常我们只会得到一个作为评分结果的矩阵MXN，这个矩阵是两个矩阵（用户-特性矩阵和特性-物品矩阵）通过某些factor作用的结果。如果这个MXN的矩阵中有缺省内容，如何通过已有信息预测空缺项？</p>
<p>用户维度为M，物品维度为N，特性维度就是latent factor的数目假设为是K。我们希望找到MXK，和KXN的矩阵，让他们相乘之后的结果与评分矩阵MXN最接近，需要minimize error。</p>
<p>对于损失函数的定义如下：Minimizing</p>
<script type="math/tex; mode=display">
L = \sum_{(i,j)}{(r^i\cdot r^j-n_{ij})^2}</script><p>Only considering the defined value and find $r^i, r^j$by gradient descent.</p>
<p>当然也可以在loss function中添加偏置项，更多其他与i有关的scale和与j有关的scale。</p>
<h1 id="Word-embedding词嵌入"><a href="#Word-embedding词嵌入" class="headerlink" title="Word-embedding词嵌入"></a>Word-embedding词嵌入</h1><p>为什么需要词嵌入？</p>
<p>传统的表示一个文字的方法就是1-of-N encoding，每一个word用一个vector来表示，这个vector的dimension就是这个世界上可能有的word的数目，很显然这过于大了。但是词汇之间是有联系的，可以组成word class，因此可以把同样性质但是不同的word用所属的class来表示。</p>
<p>词嵌入就是把每一个词都project到high dimension space上去，在这个高维空间中每一个word embedding都有一个feature vector。</p>
<p>如何找到适合该词的词嵌入？了解一个词汇的含义需要看该词的contex。</p>
<h2 id="count-based"><a href="#count-based" class="headerlink" title="count-based"></a>count-based</h2><p>基于计数的词嵌入原则是单词$w_i$的word vector是$V(w_i)$，单词$w_j$的word vector是$V(w_j)$，计算出这两个word vector的inner product后我们希望该值和两个词在该文章中同时出现的次数越近越好。其实这个思想和matrix factorization类似。</p>
<h2 id="prediction-based"><a href="#prediction-based" class="headerlink" title="prediction-based"></a>prediction-based</h2><h3 id="预测结构"><a href="#预测结构" class="headerlink" title="预测结构"></a>预测结构</h3><p>最朴素的就是拿前一个词汇去预测后一个词汇，也可以用前后词汇预测中间词汇，也可以用中间词汇预测前后词汇。</p>
<h3 id="输入输出定义"><a href="#输入输出定义" class="headerlink" title="输入输出定义"></a>输入输出定义</h3><p>希望同class的多个先后连续的词作输入的时候，输出是预测的下一个词汇。为了实现这个效果，中间hidden layer需要通过weight对输入进行转化，让他们对应到相同的空间</p>
<p>input是1-of-N encoding of the word $w_{i-2}, w_{i-1}$（需要把这两个列向量接在一起变成一个很长的vector放到neuron network中去，有点像CNN中的Flatten）。</p>
<p>output是the probability for each word as the next word $w_i$。</p>
<p>需要注意的是，两个vector对应的维度上对应到neuron network中的同一个neuron的weight是一样的（sharing parameters）。</p>
<h3 id="如何训练"><a href="#如何训练" class="headerlink" title="如何训练"></a>如何训练</h3><p>为了让$weight_{z_1}^{i-2}(1)$和$weight_{z_2}^{i-1}(1)$最终迭代结果相同，一开始就需要赋予相同的初始值，且在更新参数的时候，二者的更新表达式相同，除了Cross Entropy对自己的微分，还要算上Cross Entropy对另外一个权重的微分。</p>
<h3 id="多语言与多领域"><a href="#多语言与多领域" class="headerlink" title="多语言与多领域"></a>多语言与多领域</h3><p>需要事先知道某几对英文单词和中文单词的对应关系，把这个当作输入放到model中去learn，这个训练好的model的功能就是如何把未知的新的中文和英文对应的词汇放到一起，并投射到空间上的同一个点。</p>
<p>同样的英文和中文单词的对应可以转换成单词和图像的对应关系。</p>
<h1 id="Neighbour-Embedding"><a href="#Neighbour-Embedding" class="headerlink" title="Neighbour Embedding"></a>Neighbour Embedding</h1><p>data point可能是在高维空间中的一个manifold，实际上这笔数据完全可以放到低维空间来描述。高维空间中的问题在于如何描述两个数据的距离/相似度，用欧氏距离没有意义。</p>
<p>因而需要manifold learning把高维空间里的数据摊平到低维空间（属于非线性降维），再计算点和点之间的欧氏距离，结合后续的监督学习。</p>
<h2 id="LLE-局部线性嵌入-Locally-Linear-Embedding"><a href="#LLE-局部线性嵌入-Locally-Linear-Embedding" class="headerlink" title="LLE 局部线性嵌入 Locally Linear Embedding"></a>LLE 局部线性嵌入 Locally Linear Embedding</h2><p>原来的空间里面的点之间的关系用weight来表示，<strong>假设某一个点可以通过它的邻居的线性组合而成</strong>，因此我们需要让线性组合后的结果和原点的距离越近越好，也就是最小化所有的点的距离之和。总结：已知高维空间中的数据，求出数据之间的关系weight。</p>
<p><strong>假设从高维空间到低维空间，描述空间里点之间关系的weight是不变的，</strong>因此可以用上面求出来的weight用同样的方式定义一个相似的函数。总结：已知的是数据间的关系weight，求的是低维空间中的数据。</p>
<h2 id="LE-拉普拉斯特征映射-Laplacian-Eigenmap"><a href="#LE-拉普拉斯特征映射-Laplacian-Eigenmap" class="headerlink" title="LE 拉普拉斯特征映射 Laplacian Eigenmap"></a>LE 拉普拉斯特征映射 Laplacian Eigenmap</h2><p>在semi-supervised learning中提出过graph-based approach以及定量描述smoothness相类似，最重要的区别在于有带label的数据项去约束smoothness的表达式。</p>
<p>我们希望close in high density region中的数据投射到低维空间之后数据还是很相近，但是要对投射后的结果加一点constraint，如果一开始的维度是M，那么找出来的那些在低维空间的点做span之后还会恢复成原来数据的M维度（可逆？）。</p>
<h2 id="T-SNE"><a href="#T-SNE" class="headerlink" title="T-SNE"></a>T-SNE</h2><p>T分布随机领域 T-distributed stochastic neighbour embedding</p>
<h3 id="why"><a href="#why" class="headerlink" title="why"></a>why</h3><p>以上都假设相近的点非线性降维之后都相近，但是却没有说不相近的点非线性降维之后都不相近（有可能不相近的点线性降维之后反而相近了）。</p>
<p>之前都是针对某一个数据locally的相近，如果从整体上看呢？我们要求投影前后的数据的分布distribution相近。</p>
<h3 id="how"><a href="#how" class="headerlink" title="how"></a>how</h3><p>如何计算？</p>
<p>分别计算出所有投影前数据x之间的similarity（已知），并写出投影后数据z之间的similarity（未知），让这两个值之间的KL distance越小越好，从而可以求出未知的投影后的数据z。</p>
<p>如何定义和选择similarity？</p>
<p>分投影前数据的similarity和投影后数据的similarity考虑。</p>
<p>投影前数据的similarity用负的欧氏距离去exponent，因为距离一大，similarity就会非常小，确保了只有非常相近的点才有值。</p>
<p>投影后数据的similarity有两种方式。可以选择和投影前数据相同的衡量方式，这样总的方法就是SNE；但更好的是选择T-dirtribution中的一种用欧氏距离加上一，整体取个倒数作为similarity，这样总的方法就是T-SNE。</p>
<h1 id="自编码器-auto-encoder"><a href="#自编码器-auto-encoder" class="headerlink" title="自编码器 auto encoder"></a>自编码器 auto encoder</h1><p>想找一个编码器，input是一个比较复杂的东西，encoder就是一个neuron network，它的output就是code，维度远比输入要小实现了类似压缩的效果。</p>
<p>结构如下：input layer — layer1 — layer2 — … bottle… — layer2’  — layer1’  — output layer</p>
<p>input layer到bottle是属于encoder，bottle到output layer属于decoder，bottle的结果就是code。也就是dimension reduction的过程和reconstruct的过程。</p>
<h2 id="TIPS-on-auto-encoder"><a href="#TIPS-on-auto-encoder" class="headerlink" title="TIPS on auto-encoder"></a>TIPS on auto-encoder</h2><h3 id="Add-Noise"><a href="#Add-Noise" class="headerlink" title="Add Noise"></a>Add Noise</h3><p>de-noising auto-encoder，input加上noise之后再去encode，output要和加noise之前的原来的input越近越好。Neuron Network不仅学到了encode这件事，还学到了过滤噪音。</p>
<h3 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h3><p>一般用于参数的initialization，pre-training就能让你找到一组好的初始化参数。</p>
<p>注意一般做auto-encoder的时候，希望coder的dimension要比input dimension小，如果coder的dimensionbiinput dimension大的话，需要加一个很强的regularization，避免auto-encoder直接把input并起来再输出（也就是learn不起来）。每一个步骤得到的weight都fix住，然后再输出结果之后再backpropagation微调fine-tune一下这些weight。</p>
<p>适合于大量unlabeled data存在，少量labeled data的情况。大量unlabeled data用于得出最初的weight，少量labeled data用于fine-tune。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="文本检索-Text-Retrieval"><a href="#文本检索-Text-Retrieval" class="headerlink" title="文本检索 Text Retrieval"></a>文本检索 Text Retrieval</h3><p>把一篇文章压缩成一个code，利用降维将它表示成空间中的一个vector，现在有一个词汇用于查询，把查询的词汇也变成空间中的一个点，计算该查询次和每个document之间的内积，越大的内积代表相似程度最高。</p>
<p>如何把一个document表示成一个vector？</p>
<p>可以使用bag-of-word，vector的size就是所有lexical单词的个数，可以乘上weight代表每个单词的重要性。缺点在于缺失语义，词汇之间没有相关性。</p>
<p>可以使用auto-encoder使语义考虑进来。</p>
<h3 id="类似图像搜索-Similar-Image-Search"><a href="#类似图像搜索-Similar-Image-Search" class="headerlink" title="类似图像搜索 Similar Image Search"></a>类似图像搜索 Similar Image Search</h3><p>如果单纯比较query image和database中image的pixel的相似度的话，结果会很不好。</p>
<p>需要用一个auto-encoder把image变革一个code，再在code上面去搜寻。</p>
<h3 id="CNN-auto-encoder"><a href="#CNN-auto-encoder" class="headerlink" title="CNN auto-encoder"></a>CNN auto-encoder</h3><p>unpooling无池化，记录pooling的位置，把pooling后的值放在这个记录的位置上，其他都放0。或者也不用记录pooling位置，直接全部复制。</p>
<p>deconvolution反卷积，本质也是convolution，只不过weight相反。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML/" rel="tag"># ML</a>
              <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag"># 无监督学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/05/semi-supervised-learning/" rel="prev" title="半监督学习">
      <i class="fa fa-chevron-left"></i> 半监督学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/07/generative-model/" rel="next" title="深度生成模型">
      深度生成模型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">聚类</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dimension-Reduction%E9%99%8D%E7%BB%B4"><span class="nav-number">2.</span> <span class="nav-text">Dimension Reduction降维</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">2.1.</span> <span class="nav-text">特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Principal-Component-Analysis-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="nav-number">2.2.</span> <span class="nav-text">Principal Component Analysis 主成分分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4"><span class="nav-number">2.2.1.</span> <span class="nav-text">一维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E7%BB%B4"><span class="nav-number">2.2.2.</span> <span class="nav-text">高维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E6%AD%A3%E4%BA%A4%E5%9F%BA"><span class="nav-number">2.2.3.</span> <span class="nav-text">如何寻找正交基</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#w-1-%E7%9A%84%E5%AF%BB%E6%89%BE"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">$w^1$的寻找</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#w-2-%E7%9A%84%E5%AF%BB%E6%89%BE"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">$w^2$的寻找</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%95%E5%BD%B1%E5%90%8E%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">2.2.4.</span> <span class="nav-text">投影后数据的特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E5%8F%A6%E4%B8%80%E4%B8%AA%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3PCA"><span class="nav-number">2.2.5.</span> <span class="nav-text">从另一个角度理解PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%B1%E7%82%B9"><span class="nav-number">2.2.6.</span> <span class="nav-text">弱点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-Matrix-Factorization"><span class="nav-number">3.</span> <span class="nav-text">矩阵分解 Matrix Factorization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Word-embedding%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">4.</span> <span class="nav-text">Word-embedding词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#count-based"><span class="nav-number">4.1.</span> <span class="nav-text">count-based</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prediction-based"><span class="nav-number">4.2.</span> <span class="nav-text">prediction-based</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84"><span class="nav-number">4.2.1.</span> <span class="nav-text">预测结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%AE%9A%E4%B9%89"><span class="nav-number">4.2.2.</span> <span class="nav-text">输入输出定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83"><span class="nav-number">4.2.3.</span> <span class="nav-text">如何训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E9%A2%86%E5%9F%9F"><span class="nav-number">4.2.4.</span> <span class="nav-text">多语言与多领域</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Neighbour-Embedding"><span class="nav-number">5.</span> <span class="nav-text">Neighbour Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LLE-%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5-Locally-Linear-Embedding"><span class="nav-number">5.1.</span> <span class="nav-text">LLE 局部线性嵌入 Locally Linear Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LE-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84-Laplacian-Eigenmap"><span class="nav-number">5.2.</span> <span class="nav-text">LE 拉普拉斯特征映射 Laplacian Eigenmap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#T-SNE"><span class="nav-number">5.3.</span> <span class="nav-text">T-SNE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why"><span class="nav-number">5.3.1.</span> <span class="nav-text">why</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how"><span class="nav-number">5.3.2.</span> <span class="nav-text">how</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-auto-encoder"><span class="nav-number">6.</span> <span class="nav-text">自编码器 auto encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TIPS-on-auto-encoder"><span class="nav-number">6.1.</span> <span class="nav-text">TIPS on auto-encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-Noise"><span class="nav-number">6.1.1.</span> <span class="nav-text">Add Noise</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pre-training"><span class="nav-number">6.1.2.</span> <span class="nav-text">Pre-training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-number">6.2.</span> <span class="nav-text">应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E6%A3%80%E7%B4%A2-Text-Retrieval"><span class="nav-number">6.2.1.</span> <span class="nav-text">文本检索 Text Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E4%BC%BC%E5%9B%BE%E5%83%8F%E6%90%9C%E7%B4%A2-Similar-Image-Search"><span class="nav-number">6.2.2.</span> <span class="nav-text">类似图像搜索 Similar Image Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN-auto-encoder"><span class="nav-number">6.2.3.</span> <span class="nav-text">CNN auto-encoder</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">徐徐</p>
  <div class="site-description" itemprop="description">记录，分享。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">80</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">98</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01hY2hhQ3JvaXNzYW50" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;MachaCroissant"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnh5aHN1OTlAZ21haWwuY29t" title="E-Mail → mailto:xyhsu99@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">徐徐</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">290k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:47</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly9taXN0LnRoZW1lLW5leHQub3Jn">NexT.Mist</span> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
