<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="记录，分享。">
    <meta name="author" content="徐徐">
    
    <title>
        
            深度学习总介绍 |
        
        一通胡编
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/favicon-32x32-next.png">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/css/font-awesome.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"machacroissant.github.io","root":"/","language":"zh-CN","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/tuotuo.jpeg","favicon":"/images/favicon-32x32-next.png","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":true},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"记录，分享。"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":false},"code_copy":{"enable":true,"style":"mac"},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.3"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="来个抹茶可颂" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                一通胡编
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">深度学习总介绍</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/tuotuo.jpeg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">徐徐</span>
                        
                            <span class="author-label">抱歉选手</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;2020-11-03 08:42:45
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/deeplearning/">deeplearning</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    

    
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>9 分钟</span>
        </span>
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h1 id="深度学习三个步骤"><a href="#深度学习三个步骤" class="headerlink" title="深度学习三个步骤"></a>深度学习三个步骤</h1><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>前馈feedforward，输入进入网络后流动是单向的。两层之间的连接并没有反馈feedback。</p>
<p>全链接fully connect，每一层之间两两都有链接。</p>
<p>Input Layer输入层 1层— Hidden Layer 隐藏层 N层 — Output Layer输出层 1层。</p>
<p>Deep = many hidden layers</p>
<h2 id="Goodness-of-function"><a href="#Goodness-of-function" class="headerlink" title="Goodness of function"></a>Goodness of function</h2><h2 id="单个训练样本"><a href="#单个训练样本" class="headerlink" title="单个训练样本"></a>单个训练样本</h2><p>采用损失函数Loss function来反映模型的好差，利用交叉熵函数对$y$和$\hat{y}$的损失进行计算。</p>
<script type="math/tex; mode=display">
x_i \stackrel{\omega_i}{\longrightarrow} \dots\stackrel{Activation\,Function}{\longrightarrow} y_i \stackrel{Cross\,Entropy}{\longleftrightarrow} \hat{y}_i</script><p>注意损失函数是定义在单个训练样本上的（在表达式上使用下标，且使用小写的$l$）也就是一个样本的误差。</p>
<h2 id="所有训练样本"><a href="#所有训练样本" class="headerlink" title="所有训练样本"></a>所有训练样本</h2><script type="math/tex; mode=display">
x^i \stackrel{Neuron\, Network}{\longrightarrow} y^i \stackrel{Cross\,Entropy\, C^i}{\longrightarrow} \hat{y}^i</script><p>总体损失函数Total loss function是所有样本的误差的总和。也是反向传播需要最小化的值。</p>
<script type="math/tex; mode=display">
L = \sum_{n=1}^{n}{l_n}</script><p>注意这里的$x^i$不是一维的数据，是用来表示一个对象的多维度向量，$x^i$和$x_i$表示的对象不相同，注意区分上下标。</p>
<p>注意是吧所有训练数据的损失都加起来得到的总体损失L。为了最小化这个损失L，也就是要在function set里面找一个最优函数，也是酒找神经网络中的参数$\omega$。</p>
<h2 id="Pick-best-function"><a href="#Pick-best-function" class="headerlink" title="Pick best function"></a>Pick best function</h2><p>使用梯度下降。</p>
<p>Back Propagation: an efficient way to compute $\partial{L}/\partial{\omega}$ in neuron network.</p>
<hr>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>反向传播是在使用梯度下降计算参数变化量的时候，让求梯度更方便的一种方法。</p>
<p>反向传播背后的数学原理就是链式法则。</p>
<p>因为单个样本的loss function是关于系统输出$y$的函数，输入$x$通过系统一层层传递才到输出$y$，所以链式法则拆分的时候也是拆阶段性结果做微分。</p>
<h2 id="单个训练样本分析"><a href="#单个训练样本分析" class="headerlink" title="单个训练样本分析"></a>单个训练样本分析</h2><p><img src="/2020/11/03/back-propagation/onesample.png" alt="onesample" style="zoom:50%;"></p>
<p>输入先进入一个由权重weight和偏移b组成的线性系统获得线性函数输出z。</p>
<p>线性输出z进入一个激励函数non-linear activation function获得一个非线性输出，该输出作为下一层神经网络的输入。最常用的非线性激励函数就是Sigmoid Function。</p>
<blockquote>
<p>为什么需要激励函数？用于加入非线性因素。因为有些数据线性可分，但是某些数据线性不可分。</p>
<p><a class="link" target="_blank" rel="noopener" href="https://www.zhihu.com/question/22334626/answer/103835591">神经网络激励函数的作用是什么？有没有形象的解释？ - 颜沁睿的回答 - 知乎<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial \omega} = \frac{\partial l}{\partial z} \frac{\partial z}{\partial \omega}</script><p>计算$\frac{\partial z}{\partial \omega} $就是Forward Pass的过程，结果就是the value of the input connected by the weight，该层神经网络的输入。此处需要注意z已经在forward pass中被确定了。</p>
<p>计算$\frac{\partial l}{\partial z}$就是Backward Pass的过程。运用链式法则的时候需要注意，最终结果$\partial l$是和$\partial y$结合在一起的，而$\partial y$是作为non-linear activation function的输出存在的，而该非线性激励函数的输入又是$z$，即该层神经网络的输入经过线性变换之后得到值。</p>
<h2 id="Mini-Batch"><a href="#Mini-Batch" class="headerlink" title="Mini-Batch"></a>Mini-Batch</h2><p>相关参数<code>batch_size</code>和<code>nb_epoch</code>。</p>
<p>实际操作并不总是对所有样本最小化总的损失函数，而是将数据随机分成几个mini-batch，每个batch的batch_size指定之后，可以根据有多少样本算出有多少个batch。</p>
<p>初始化神经网络参数后，随机选择第一个bacth的样本，对它计算total loss，然后更新神经网络参数；第二个batch..……直到选了batch_size个batch的样本，也就是对神经网络的参数更新了(总样本数/batch_size)次，才能称做遍历了一次nb_epoch。</p>
<p>batch_size代表一个batch有多大(就是把100个example，放到一个batch里)；nb_epoch等于20表示对每个batch重复20次。</p>
<h1 id="Tips-for-Deep-Learning"><a href="#Tips-for-Deep-Learning" class="headerlink" title="Tips for Deep Learning"></a>Tips for Deep Learning</h1><p>在完成深度学习的三个步骤，得到神经网络之后，需要首先考虑是否能在训练集上获得好的表现。</p>
<p>如果在训练集上不能获得好的表现，需要从Adapative Learning Rate和New Activation Function两方面考虑。</p>
<p>如果在训练集上表现良好，但是在测试集上表现差，说明是overfitting，从Early Stopping和Regularization以及Dropout三方面考虑。</p>
<h2 id="训练集表现不好"><a href="#训练集表现不好" class="headerlink" title="训练集表现不好"></a>训练集表现不好</h2><h3 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h3><p>梯度消失是在使用Sigmoid Function作为激励函数时存在的问题。</p>
<p>依据Sigmoid Function的图像来看，它将输入输出都限定在0～1范围内，随着输入增大靠近一条渐近线。</p>
<p>当网络比较深，Sigmoid Function的输入值比较大的情况下，每一次对输入值做的变动delta，都会在输出上表现为很小的变动delta，从而靠后的hidden layer对loss的影响非常小。</p>
<hr>
<p>如何解决梯度消失的问题？</p>
<p>有两个方式，一是动态调整学习率，二是直接更改激励函数。</p>
<hr>
<h3 id="更改激励函数"><a href="#更改激励函数" class="headerlink" title="更改激励函数"></a>更改激励函数</h3><h4 id="ReLU激励函数"><a href="#ReLU激励函数" class="headerlink" title="ReLU激励函数"></a>ReLU激励函数</h4><p>input&gt;0, output=input; input&lt;=0, output=0.</p>
<p>对于input&gt;0的范围，在input比较大且变化比较的地方，梯度下降比较快，可以处理梯度下降问题。</p>
<p>对于input&lt;=0的范围，那些output=0的部分直接可以从整个网络中拿走。最后整个网络就变成了thinner linear network。</p>
<h4 id="改进1-Leaky-ReLu"><a href="#改进1-Leaky-ReLu" class="headerlink" title="改进1: Leaky ReLu"></a>改进1: Leaky ReLu</h4><h4 id="改进2-Parametric-ReLU"><a href="#改进2-Parametric-ReLU" class="headerlink" title="改进2: Parametric ReLU"></a>改进2: Parametric ReLU</h4><p>以上改进都是在input&lt;=0的情况下通过乘以一个比较小的参数让output有一点值。</p>
<h4 id="改进3-Maxout-Learnable-Activation-Function"><a href="#改进3-Maxout-Learnable-Activation-Function" class="headerlink" title="改进3: Maxout/Learnable Activation Function"></a>改进3: Maxout/Learnable Activation Function</h4><p>应该是先对每一层的neuron进行group，每一个group后的结果z都是max{group member result z}，对于所有input的可能取值，就相当于线性规划出了一个linear convex function。group的数量越多，那么convex的角就越多，激励函数也就越复杂，更直观地变成了非线性。</p>
<p>该如何训练该网络？一方面，max过后，该网络就相当于剪枝了，另外一个线性变化后的z就不需要考虑了，最终还是得到thinner linear function。另一方面，对于较全的训练集input，应当max函数中的每一个weight和bias过后的z都会成为max过后有效网络的一部分。</p>
<h3 id="动态调整学习率"><a href="#动态调整学习率" class="headerlink" title="动态调整学习率"></a>动态调整学习率</h3><h4 id="PMSProp-Root-Mean-Square-Prop"><a href="#PMSProp-Root-Mean-Square-Prop" class="headerlink" title="PMSProp/Root Mean Square Prop"></a>PMSProp/Root Mean Square Prop</h4><blockquote>
<p>Adagrad中也提出了动态调整学习率，Use first drivative to estimate second derivative。用固定的learning rate除以这个参数过去所有GD值的平方和开根号。</p>
<script type="math/tex; mode=display">
\omega^{t+1} \larr \omega^{t} - \frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t</script></blockquote>
<p>Root Mean Square of the gradients with previous gradients being decayed.</p>
<script type="math/tex; mode=display">
\omega^{t+1} \larr \omega^{t} - \frac{\eta}{\sigma^t}g^t \\
\sigma^t = \sqrt{\alpha(\sigma^{t-1})^2+(1-\alpha)(g^t)^2}</script><p>$\alpha$值调的小一点，说明倾向于相信新的gradient指示的error surface的平滑程度/陡峭程度。</p>
<h3 id="local-minima的解决"><a href="#local-minima的解决" class="headerlink" title="local minima的解决"></a>local minima的解决</h3><p>当神经网络很大，参数越多，出现local minima的几率越低。</p>
<p>联系物理中的惯性，每次移动的方向，不仅考虑某一点的gradient，还考虑前一个时间点移动的方向movement of last step，将这两者求矢量和。</p>
<p>对于某一个点$\theta^i$，此处有movement为$v^i$，对该点计算gradient得到$\nabla{L(\theta^i)}$，那么下一次移动的方向movement为$v^{i+1} = \lambda v^i - \eta \nabla{L(\theta^i)} $。如果递推下去，可以有</p>
<script type="math/tex; mode=display">
v^{i+1} = \lambda{v^i} - \eta \nabla{L(\theta^i)}  \\
\begin{split}
v^{i+2} &= \lambda{v^{i+1}} - \eta \nabla{L(\theta^{i+1})} \\ 
&= \lambda{(\lambda{v^i} - \eta \nabla{L(\theta^i)})}- \eta \nabla{L(\theta^{i+1})} \\
&= \lambda^2 v_i -\lambda\eta\nabla{L(\theta_i)} - \eta \nabla{L(\theta^{i+1})} 

\end{split}</script><p>$v^i$ is actually the weighted sum of all the previous gradient.越之前的gradient，对此刻的movement影响越小，越多考虑目前的gradient的影响。</p>
<p>Adam = RMSProp + Momentum</p>
<h2 id="测试集表现不好"><a href="#测试集表现不好" class="headerlink" title="测试集表现不好"></a>测试集表现不好</h2><p>注意这个是在训练集表现良好的基础之上进行的。</p>
<p>此处的测试集是cross validation中从原训练集中抽取出来的validation set。</p>
<h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>虽然训练集在loss降低，但测试集上有可能loss降低后又升高了。需要让epoch停留在测试集的最低点。</p>
<h3 id="Regulariazation"><a href="#Regulariazation" class="headerlink" title="Regulariazation"></a>Regulariazation</h3><p>通过给原损失函数添加一个Regularization Term，构成一个新的需要最小化的损失函数。目的是weight decay，closer to zero。</p>
<h4 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h4><p>Regularization Term = $\lambda \frac{1}{2}  \left|\theta\right|_2$</p>
<p>$\left|\theta\right|_2 = (\omega_1)^2 + (\omega_2)^2 + \cdots$</p>
<p>对新的损失函数求微分且合并同类项后可以发现，对已有weight$\omega^t$总是乘上总小于1的$(1-\eta\lambda)$，相比于之前的乘以1，目的是让weight更接近0。是基于已有weight的。</p>
<h4 id="L1-Regularization"><a href="#L1-Regularization" class="headerlink" title="L1 Regularization"></a>L1 Regularization</h4><p>Regularization Term = $\lambda \frac{1}{2}  \left|\theta\right|_1$</p>
<p>$\left|\theta\right|_1 = |\omega_1| + |\omega_2| + \cdots$</p>
<p>对新的损失函数求微分后发现不可合并同类项，只是减了一个$\eta\lambda sgn(\omega^t)$，每次减的都是固定值。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>训练时，每次在更新参数之前，让每个neuron都有p%的几率dropout，然后使用new thinner network去训练。for each mini-batch, resample the dropout neurons。在加上dropout之后，training set上的测试结果会变差。</p>
<p>测试时，不做dropout，训练时的dropout rate是p%，则测试时所有的weight都乘以(1-p)%。</p>
<hr>
<p>为什么dropout会有用？</p>
<p>Dropout is a kind of ensemble. 就相当于把训练集分成了好多个子集，每个子集都生成一个network。当把testing data都分别放入这些网络的时候，每个网络都会给出一个结果，最终给这些结果去一个平均值。</p>
<h1 id="why-deep"><a href="#why-deep" class="headerlink" title="why deep"></a>why deep</h1><p>深度学习其实就是模组化。</p>
<p>每一层neural可以被看做是一个basic classifier，第一层的neural就是最基分类器；第二层的neural是比较复杂的classifier，把第一层basic classifier 的output当做第二层的input(把第一层的classifier当做module)，第三层把第二层当做module，以此类推。</p>
<p>例子：语音辨识/逻辑电路</p>
<p>similar input — different output / different output — similar output</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li>本文标题：深度学习总介绍</li>
        <li>本文作者：徐徐</li>
        <li>创建时间：2020-11-03 08:42:45</li>
        <li>
            本文链接：https://machacroissant.github.io/2020/11/03/back-propagation/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2020/11/03/markov-chain/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">马尔可夫链</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2020/10/30/sketch-app-design-workflow/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">使用sketch设计商城app原型的流程</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;评论</i>
    </div>
    

        
            
    <div class="valine-container">
        <script 
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script >
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'aqAOPQAx86speqYXI4gPuLWx-gzGzoHsz',
                    appKey: '8fzaXFq8Ikhr5mucbhj3qqUS',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'Let&#39;s talk!',
                    lang: 'zh-CN'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = '徐徐';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('false') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>&nbsp;-&nbsp;
            
            2021&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">徐徐</a>
        </div>
        
            <script async  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                
                    <span id="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.3</a>
        </div>
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%89%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.</span> <span class="nav-text">深度学习三个步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network"><span class="nav-number">1.1.</span> <span class="nav-text">Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Goodness-of-function"><span class="nav-number">1.2.</span> <span class="nav-text">Goodness of function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC"><span class="nav-number">1.3.</span> <span class="nav-text">单个训练样本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%80%E6%9C%89%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC"><span class="nav-number">1.4.</span> <span class="nav-text">所有训练样本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pick-best-function"><span class="nav-number">1.5.</span> <span class="nav-text">Pick best function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E5%88%86%E6%9E%90"><span class="nav-number">2.1.</span> <span class="nav-text">单个训练样本分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-Batch"><span class="nav-number">2.2.</span> <span class="nav-text">Mini-Batch</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tips-for-Deep-Learning"><span class="nav-number">3.</span> <span class="nav-text">Tips for Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E8%A1%A8%E7%8E%B0%E4%B8%8D%E5%A5%BD"><span class="nav-number">3.1.</span> <span class="nav-text">训练集表现不好</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-Gradient-Problem"><span class="nav-number">3.1.1.</span> <span class="nav-text">Vanishing Gradient Problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%94%B9%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.2.</span> <span class="nav-text">更改激励函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">ReLU激励函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B1-Leaky-ReLu"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">改进1: Leaky ReLu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B2-Parametric-ReLU"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">改进2: Parametric ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B3-Maxout-Learnable-Activation-Function"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">改进3: Maxout&#x2F;Learnable Activation Function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">3.1.3.</span> <span class="nav-text">动态调整学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PMSProp-Root-Mean-Square-Prop"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">PMSProp&#x2F;Root Mean Square Prop</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#local-minima%E7%9A%84%E8%A7%A3%E5%86%B3"><span class="nav-number">3.1.4.</span> <span class="nav-text">local minima的解决</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E9%9B%86%E8%A1%A8%E7%8E%B0%E4%B8%8D%E5%A5%BD"><span class="nav-number">3.2.</span> <span class="nav-text">测试集表现不好</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Early-stopping"><span class="nav-number">3.2.1.</span> <span class="nav-text">Early stopping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regulariazation"><span class="nav-number">3.2.2.</span> <span class="nav-text">Regulariazation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-Regularization"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">L2 Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L1-Regularization"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">L1 Regularization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">3.2.3.</span> <span class="nav-text">Dropout</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#why-deep"><span class="nav-number">4.</span> <span class="nav-text">why deep</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/dark-light-toggle.js"></script>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/local-search.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/code-copy.js"></script>




<div class="post-scripts">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/left-side-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.3/source/js/toc.js"></script>
    
</div>



</body>
</html>
